# Google Gemini Safety Settings

## Overview
Gemini's safety settings provide comprehensive content filtering and harm prevention capabilities. These built-in protections help ensure responsible AI usage by filtering potentially harmful content across multiple categories.

## Safety Categories

### Core Harm Categories
1. **Harassment**: Content that bullies, intimidates, or targets individuals
2. **Hate Speech**: Content that promotes hatred or discrimination against groups
3. **Sexually Explicit**: Adult sexual content and explicit material
4. **Dangerous Content**: Content that promotes harmful or dangerous activities
5. **Civic Integrity**: Content related to elections, political processes, and civic participation

### Safety Levels
Each category is rated with probability levels:
- **NEGLIGIBLE**: Very low probability of harm
- **LOW**: Low probability of harm
- **MEDIUM**: Medium probability of harm
- **HIGH**: High probability of harm

## Blocking Thresholds

### Available Thresholds
- **BLOCK_NONE**: Allow all content (not recommended for production)
- **BLOCK_FEW**: Block only high-probability unsafe content
- **BLOCK_SOME**: Block medium and high probability content (default)
- **BLOCK_MOST**: Block low, medium, and high probability content

### Default Settings
```python
# Default safety settings (BLOCK_SOME for all categories)
default_safety = [
    {
        \"category\": \"HARM_CATEGORY_HARASSMENT\",\n        \"threshold\": \"BLOCK_SOME\"\n    },\n    {\n        \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n        \"threshold\": \"BLOCK_SOME\"\n    },\n    {\n        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n        \"threshold\": \"BLOCK_SOME\"\n    },\n    {\n        \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n        \"threshold\": \"BLOCK_SOME\"\n    }\n]\n```\n\n## Configuration Examples\n\n### Basic Safety Configuration\n```python\nimport google.generativeai as genai\n\n# Configure model with custom safety settings\nsafety_settings = [\n    {\n        \"category\": \"HARM_CATEGORY_HARASSMENT\",\n        \"threshold\": \"BLOCK_MOST\"\n    },\n    {\n        \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n        \"threshold\": \"BLOCK_MOST\"\n    },\n    {\n        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n        \"threshold\": \"BLOCK_SOME\"\n    },\n    {\n        \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n        \"threshold\": \"BLOCK_SOME\"\n    }\n]\n\nmodel = genai.GenerativeModel(\n    'gemini-2.5-flash',\n    safety_settings=safety_settings\n)\n```\n\n### Restrictive Safety Settings\n```python\n# Maximum safety for sensitive applications\nmax_safety_settings = [\n    {\n        \"category\": \"HARM_CATEGORY_HARASSMENT\",\n        \"threshold\": \"BLOCK_MOST\"\n    },\n    {\n        \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n        \"threshold\": \"BLOCK_MOST\"\n    },\n    {\n        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n        \"threshold\": \"BLOCK_MOST\"\n    },\n    {\n        \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n        \"threshold\": \"BLOCK_MOST\"\n    }\n]\n\n# Use for applications with children or sensitive content\nmodel = genai.GenerativeModel(\n    'gemini-2.5-flash',\n    safety_settings=max_safety_settings\n)\n```\n\n### Permissive Settings (Research/Academic)\n```python\n# More permissive for research or academic applications\nresearch_safety_settings = [\n    {\n        \"category\": \"HARM_CATEGORY_HARASSMENT\",\n        \"threshold\": \"BLOCK_FEW\"\n    },\n    {\n        \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n        \"threshold\": \"BLOCK_FEW\"\n    },\n    {\n        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n        \"threshold\": \"BLOCK_SOME\"\n    },\n    {\n        \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n        \"threshold\": \"BLOCK_SOME\"\n    }\n]\n\n# Note: Use with caution and appropriate oversight\nmodel = genai.GenerativeModel(\n    'gemini-2.5-flash',\n    safety_settings=research_safety_settings\n)\n```\n\n## Handling Safety Blocks\n\n### Detecting Safety Blocks\n```python\ndef handle_safety_response(prompt: str):\n    \"\"\"Generate content with safety handling.\"\"\"\n    try:\n        response = model.generate_content(prompt)\n        \n        # Check if response was blocked\n        if response.candidates[0].finish_reason == \"SAFETY\":\n            # Get safety ratings\n            safety_ratings = response.candidates[0].safety_ratings\n            \n            blocked_categories = []\n            for rating in safety_ratings:\n                if rating.probability in [\"MEDIUM\", \"HIGH\"]:\n                    blocked_categories.append(rating.category)\n            \n            return {\n                \"status\": \"blocked\",\n                \"reason\": \"Content blocked due to safety concerns\",\n                \"categories\": blocked_categories,\n                \"content\": None\n            }\n        \n        return {\n            \"status\": \"success\",\n            \"content\": response.text,\n            \"safety_ratings\": response.candidates[0].safety_ratings\n        }\n        \n    except Exception as e:\n        return {\n            \"status\": \"error\",\n            \"reason\": str(e),\n            \"content\": None\n        }\n\n# Usage\nresult = handle_safety_response(\"Tell me about online safety for children\")\nif result[\"status\"] == \"blocked\":\n    print(f\"Content blocked: {result['categories']}\")\nelse:\n    print(result[\"content\"])\n```\n\n### Retry with Modified Prompt\n```python\ndef safe_content_generation(original_prompt: str, max_retries: int = 3):\n    \"\"\"Generate content with automatic prompt modification on safety blocks.\"\"\"\n    \n    prompt_variations = [\n        original_prompt,\n        f\"Please provide educational information about: {original_prompt}\",\n        f\"From an academic perspective, explain: {original_prompt}\",\n        f\"Provide factual, objective information about: {original_prompt}\"\n    ]\n    \n    for i, prompt in enumerate(prompt_variations[:max_retries]):\n        try:\n            response = model.generate_content(prompt)\n            \n            if response.candidates[0].finish_reason != \"SAFETY\":\n                return {\n                    \"success\": True,\n                    \"content\": response.text,\n                    \"attempts\": i + 1\n                }\n        except Exception as e:\n            continue\n    \n    return {\n        \"success\": False,\n        \"reason\": \"All attempts blocked by safety filters\",\n        \"attempts\": max_retries\n    }\n```\n\n## Use Case-Specific Configurations\n\n### Educational Applications\n```python\n# Safety settings for educational content\neducational_safety = [\n    {\n        \"category\": \"HARM_CATEGORY_HARASSMENT\",\n        \"threshold\": \"BLOCK_MOST\"\n    },\n    {\n        \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n        \"threshold\": \"BLOCK_MOST\"\n    },\n    {\n        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n        \"threshold\": \"BLOCK_MOST\"\n    },\n    {\n        \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n        \"threshold\": \"BLOCK_SOME\"  # Allow some educational dangerous content\n    }\n]\n\ndef create_educational_content(topic: str):\n    \"\"\"Generate educational content with appropriate safety.\"\"\"\n    model = genai.GenerativeModel(\n        'gemini-2.5-flash',\n        safety_settings=educational_safety,\n        system_instruction=\"You are an educational assistant. Provide factual, age-appropriate information.\"\n    )\n    \n    return model.generate_content(f\"Explain {topic} in an educational context\")\n```\n\n### Business Applications\n```python\n# Professional/business safety settings\nbusiness_safety = [\n    {\n        \"category\": \"HARM_CATEGORY_HARASSMENT\",\n        \"threshold\": \"BLOCK_SOME\"\n    },\n    {\n        \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n        \"threshold\": \"BLOCK_SOME\"\n    },\n    {\n        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n        \"threshold\": \"BLOCK_MOST\"\n    },\n    {\n        \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n        \"threshold\": \"BLOCK_SOME\"\n    }\n]\n\ndef generate_business_content(prompt: str):\n    \"\"\"Generate content for business use cases.\"\"\"\n    model = genai.GenerativeModel(\n        'gemini-2.5-flash',\n        safety_settings=business_safety,\n        system_instruction=\"You are a professional business assistant. Maintain appropriate, professional tone.\"\n    )\n    \n    return model.generate_content(prompt)\n```\n\n### Customer Service Applications\n```python\n# Customer service safety settings\ncustomer_service_safety = [\n    {\n        \"category\": \"HARM_CATEGORY_HARASSMENT\",\n        \"threshold\": \"BLOCK_MOST\"\n    },\n    {\n        \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n        \"threshold\": \"BLOCK_MOST\"\n    },\n    {\n        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n        \"threshold\": \"BLOCK_MOST\"\n    },\n    {\n        \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n        \"threshold\": \"BLOCK_MOST\"\n    }\n]\n\nclass SafeCustomerServiceBot:\n    def __init__(self):\n        self.model = genai.GenerativeModel(\n            'gemini-2.5-flash',\n            safety_settings=customer_service_safety,\n            system_instruction=\"You are a helpful, professional customer service representative.\"\n        )\n    \n    def respond_to_customer(self, customer_message: str):\n        \"\"\"Generate safe customer service response.\"\"\"\n        try:\n            response = self.model.generate_content(\n                f\"Customer message: {customer_message}\\n\\nProvide a helpful, professional response.\"\n            )\n            \n            if response.candidates[0].finish_reason == \"SAFETY\":\n                return \"I understand you have a concern. Let me connect you with a human representative who can better assist you.\"\n            \n            return response.text\n            \n        except Exception:\n            return \"I apologize, but I'm having trouble processing your request. Please contact our support team directly.\"\n```\n\n## Safety Monitoring and Analytics\n\n### Safety Metrics Tracking\n```python\nclass SafetyMonitor:\n    def __init__(self):\n        self.total_requests = 0\n        self.blocked_requests = 0\n        self.safety_violations = {\n            \"HARM_CATEGORY_HARASSMENT\": 0,\n            \"HARM_CATEGORY_HATE_SPEECH\": 0,\n            \"HARM_CATEGORY_SEXUALLY_EXPLICIT\": 0,\n            \"HARM_CATEGORY_DANGEROUS_CONTENT\": 0\n        }\n    \n    def track_request(self, response):\n        \"\"\"Track safety metrics for requests.\"\"\"\n        self.total_requests += 1\n        \n        if response.candidates[0].finish_reason == \"SAFETY\":\n            self.blocked_requests += 1\n            \n            # Track which categories caused the block\n            for rating in response.candidates[0].safety_ratings:\n                if rating.probability in [\"MEDIUM\", \"HIGH\"]:\n                    category = rating.category.name\n                    if category in self.safety_violations:\n                        self.safety_violations[category] += 1\n    \n    def get_safety_stats(self):\n        \"\"\"Get safety statistics.\"\"\"\n        if self.total_requests == 0:\n            return {}\n        \n        return {\n            \"total_requests\": self.total_requests,\n            \"blocked_requests\": self.blocked_requests,\n            \"block_rate\": self.blocked_requests / self.total_requests,\n            \"category_violations\": self.safety_violations,\n            \"most_common_violation\": max(self.safety_violations, key=self.safety_violations.get)\n        }\n\n# Usage\nmonitor = SafetyMonitor()\n\ndef monitored_generation(prompt: str):\n    response = model.generate_content(prompt)\n    monitor.track_request(response)\n    return response\n```\n\n### Safety Dashboard\n```python\nclass SafetyDashboard:\n    def __init__(self):\n        self.monitor = SafetyMonitor()\n        self.flagged_prompts = []\n    \n    def safe_generate(self, prompt: str, user_id: str = None):\n        \"\"\"Generate with safety monitoring and logging.\"\"\"\n        response = model.generate_content(prompt)\n        self.monitor.track_request(response)\n        \n        if response.candidates[0].finish_reason == \"SAFETY\":\n            self.flagged_prompts.append({\n                \"timestamp\": datetime.now(),\n                \"prompt\": prompt[:100] + \"...\" if len(prompt) > 100 else prompt,\n                \"user_id\": user_id,\n                \"safety_ratings\": response.candidates[0].safety_ratings\n            })\n        \n        return response\n    \n    def generate_safety_report(self):\n        \"\"\"Generate comprehensive safety report.\"\"\"\n        stats = self.monitor.get_safety_stats()\n        \n        report = f\"\"\"\n        SAFETY REPORT\n        =============\n        Total Requests: {stats.get('total_requests', 0)}\n        Blocked Requests: {stats.get('blocked_requests', 0)}\n        Block Rate: {stats.get('block_rate', 0):.2%}\n        \n        Category Violations:\n        \"\"\"\n        \n        for category, count in stats.get('category_violations', {}).items():\n            report += f\"\\n- {category}: {count}\"\n        \n        report += f\"\\n\\nRecent Flagged Prompts: {len(self.flagged_prompts)}\"\n        \n        return report\n```\n\n## Advanced Safety Strategies\n\n### Multi-Layer Safety Approach\n```python\nclass MultiLayerSafety:\n    def __init__(self):\n        # Different safety configurations for different layers\n        self.strict_safety = [\n            {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MOST\"},\n            {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MOST\"},\n            {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_MOST\"},\n            {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_MOST\"}\n        ]\n        \n        self.moderate_safety = [\n            {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_SOME\"},\n            {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_SOME\"},\n            {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_SOME\"},\n            {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_SOME\"}\n        ]\n    \n    def pre_filter_prompt(self, prompt: str) -> bool:\n        \"\"\"Pre-filter prompts using keyword detection.\"\"\"\n        dangerous_keywords = ['bomb', 'weapon', 'hack', 'illegal']\n        return not any(keyword in prompt.lower() for keyword in dangerous_keywords)\n    \n    def safe_generate(self, prompt: str, user_context: dict = None):\n        \"\"\"Generate with multiple safety layers.\"\"\"\n        # Layer 1: Pre-filtering\n        if not self.pre_filter_prompt(prompt):\n            return {\"error\": \"Prompt flagged by pre-filter\", \"content\": None}\n        \n        # Layer 2: Choose safety level based on context\n        safety_settings = self.strict_safety\n        if user_context and user_context.get('verified_researcher'):\n            safety_settings = self.moderate_safety\n        \n        # Layer 3: Gemini safety filters\n        model = genai.GenerativeModel('gemini-2.5-flash', safety_settings=safety_settings)\n        \n        try:\n            response = model.generate_content(prompt)\n            \n            if response.candidates[0].finish_reason == \"SAFETY\":\n                return {\"error\": \"Content blocked by safety filters\", \"content\": None}\n            \n            # Layer 4: Post-generation filtering\n            if self.post_filter_response(response.text):\n                return {\"success\": True, \"content\": response.text}\n            else:\n                return {\"error\": \"Response flagged by post-filter\", \"content\": None}\n                \n        except Exception as e:\n            return {\"error\": f\"Generation failed: {str(e)}\", \"content\": None}\n    \n    def post_filter_response(self, response: str) -> bool:\n        \"\"\"Additional filtering of generated responses.\"\"\"\n        # Custom post-processing filters\n        inappropriate_patterns = ['contact me privately', 'bypass safety']\n        return not any(pattern in response.lower() for pattern in inappropriate_patterns)\n```\n\n### Context-Aware Safety\n```python\nclass ContextAwareSafety:\n    def __init__(self):\n        self.safety_profiles = {\n            'children': {\n                'harassment': 'BLOCK_MOST',\n                'hate_speech': 'BLOCK_MOST',\n                'sexually_explicit': 'BLOCK_MOST',\n                'dangerous_content': 'BLOCK_MOST'\n            },\n            'general': {\n                'harassment': 'BLOCK_SOME',\n                'hate_speech': 'BLOCK_SOME',\n                'sexually_explicit': 'BLOCK_SOME',\n                'dangerous_content': 'BLOCK_SOME'\n            },\n            'research': {\n                'harassment': 'BLOCK_FEW',\n                'hate_speech': 'BLOCK_FEW',\n                'sexually_explicit': 'BLOCK_SOME',\n                'dangerous_content': 'BLOCK_FEW'\n            }\n        }\n    \n    def get_safety_settings(self, user_profile: str):\n        \"\"\"Get safety settings based on user profile.\"\"\"\n        profile = self.safety_profiles.get(user_profile, self.safety_profiles['general'])\n        \n        return [\n            {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": profile['harassment']},\n            {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": profile['hate_speech']},\n            {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": profile['sexually_explicit']},\n            {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": profile['dangerous_content']}\n        ]\n    \n    def generate_safe_content(self, prompt: str, user_profile: str):\n        \"\"\"Generate content with profile-appropriate safety.\"\"\"\n        safety_settings = self.get_safety_settings(user_profile)\n        \n        model = genai.GenerativeModel(\n            'gemini-2.5-flash',\n            safety_settings=safety_settings\n        )\n        \n        return model.generate_content(prompt)\n```\n\n## Best Practices\n\n### Safety Configuration Guidelines\n1. **Default to restrictive**: Start with BLOCK_MOST and relax as needed\n2. **Context-appropriate**: Adjust settings based on use case and audience\n3. **Regular review**: Periodically review and update safety settings\n4. **User feedback**: Collect feedback on blocked content appropriateness\n5. **Compliance**: Ensure settings meet legal and regulatory requirements\n\n### Implementation Recommendations\n1. **Graceful handling**: Provide helpful messages when content is blocked\n2. **Alternative approaches**: Offer ways to rephrase or modify requests\n3. **Human escalation**: Provide paths to human review for blocked content\n4. **Monitoring**: Track safety metrics and trends\n5. **Documentation**: Document safety decisions and rationale\n\n### Error Communication\n```python\ndef user_friendly_safety_message(safety_ratings):\n    \"\"\"Generate user-friendly messages for safety blocks.\"\"\"\n    messages = {\n        \"HARM_CATEGORY_HARASSMENT\": \"This request might contain content that could be harmful to others. Please try rephrasing your question.\",\n        \"HARM_CATEGORY_HATE_SPEECH\": \"This request contains language that might be offensive. Please rephrase in a more respectful way.\",\n        \"HARM_CATEGORY_SEXUALLY_EXPLICIT\": \"This request appears to be asking for adult content. Please modify your request.\",\n        \"HARM_CATEGORY_DANGEROUS_CONTENT\": \"This request might involve dangerous activities. Please ask about safer alternatives.\"\n    }\n    \n    blocked_categories = []\n    for rating in safety_ratings:\n        if rating.probability in [\"MEDIUM\", \"HIGH\"]:\n            category = rating.category.name\n            if category in messages:\n                return messages[category]\n    \n    return \"I'm not able to help with this request. Please try rephrasing or asking something else.\"\n```\n\n## Compliance and Legal Considerations\n\n### Regulatory Compliance\n1. **COPPA compliance**: Enhanced protection for children under 13\n2. **GDPR considerations**: Privacy and data protection requirements\n3. **Industry standards**: Sector-specific safety requirements\n4. **Regional laws**: Local content regulations and restrictions\n5. **Accessibility**: Ensure safety measures don't discriminate\n\n### Documentation Requirements\n1. **Safety policies**: Document safety approach and reasoning\n2. **Incident logs**: Track and document safety incidents\n3. **Review processes**: Regular safety setting reviews\n4. **User guidelines**: Clear communication about content policies\n5. **Audit trails**: Maintain records for compliance purposes\n\n## Limitations and Considerations\n\n### Built-in Protections\n- **Core safety**: Some protections cannot be disabled (e.g., child safety)\n- **Model training**: Safety is built into model training, not just filtering\n- **Context awareness**: Safety considers full conversation context\n- **Continuous improvement**: Safety systems are regularly updated\n\n### Implementation Notes\n1. **Performance impact**: Minimal impact on response times\n2. **False positives**: Some legitimate content may be blocked\n3. **Evolving standards**: Safety standards may change over time\n4. **Cultural context**: Consider cultural differences in content appropriateness\n5. **Edge cases**: Plan for unusual or edge case scenarios\n\n---\n\n**Last Updated:** Based on Google Gemini API documentation as of 2025\n**Reference:** https://ai.google.dev/gemini-api/docs/safety-settings